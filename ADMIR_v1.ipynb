{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADMIR_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surajsrivathsa/image_registration/blob/main/ADMIR_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uENFUaxrdBRN"
      },
      "source": [
        "# Preamble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_QQG0blca17",
        "outputId": "2b183502-7a22-447d-c1c7-0452eef90ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install --upgrade nibabel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nibabel\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/7f/d3c29792fae50ef4f1f8f87af8a94d5d9fe76550b86ebcf8a251110169d8/nibabel-3.2.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from nibabel) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=14.3 in /usr/local/lib/python3.6/dist-packages (from nibabel) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=14.3->nibabel) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from packaging>=14.3->nibabel) (1.15.0)\n",
            "Installing collected packages: nibabel\n",
            "  Found existing installation: nibabel 3.0.2\n",
            "    Uninstalling nibabel-3.0.2:\n",
            "      Successfully uninstalled nibabel-3.0.2\n",
            "Successfully installed nibabel-3.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rT9Nd7ScgDi",
        "outputId": "9fa48b62-6a6b-49c8-a9ce-b36ddd7dd73c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-x6D4OnafOj",
        "outputId": "dc876741-6fdd-490a-8992-e06ba04ddc5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import nibabel as nb\n",
        "import os, sys, glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "print(\"nibabel version: {}\".format(nb.__version__))\n",
        "print(\"pytorch version: {}\".format(torch.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nibabel version: 3.2.0\n",
            "pytorch version: 1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O09OgvEJc_CM"
      },
      "source": [
        "t1_fn = '/content/drive/My Drive/Colab Notebooks/ImageRegistrationUsingDeepLearning/ADMIR/Dataset/IXI002-Guys-0828-T1_resampled.nii.gz'\n",
        "t2_fn = '/content/drive/My Drive/Colab Notebooks/ImageRegistrationUsingDeepLearning/ADMIR/Dataset/IXI002-Guys-0828-T2_resampled.nii.gz'  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWcFQJ_Sc-9p"
      },
      "source": [
        "data_path = \"/content/drive/My Drive/Colab Notebooks/ImageRegistrationUsingDeepLearning/ADMIR/Dataset/\"\n",
        "file_names = glob.glob(os.path.join(data_path, \"*.nii.gz\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpv76iqJoJ3C",
        "outputId": "18b8471b-e94a-44e2-ed30-7b6418d31d07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(file_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAVJGb_KdFbx"
      },
      "source": [
        "# Image Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aKmg-UYdJmK",
        "outputId": "33742c24-46c6-4b21-a34d-882da80c7985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "img_nb1 = nb.load(file_names[0])\n",
        "img_nb1.shape\n",
        "img_nb2 = nb.load(file_names[1])\n",
        "img_nb2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-819526b80e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_nb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg_nb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg_nb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg_nb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1Q1VnK3rD-n"
      },
      "source": [
        "def load_4D(name):\n",
        "    X_nb = nb.load(name)\n",
        "    X_np = X_nb.dataobj\n",
        "    X_np = np.reshape(X_np, (1,)+ X_np.shape)\n",
        "    return X_np\n",
        "\n",
        "def imgnorm(N_I,index1=0.0001,index2=0.0001):\n",
        "    I_sort = np.sort(N_I.flatten())\n",
        "    I_min = I_sort[int(index1*len(I_sort))]\n",
        "    I_max = I_sort[-int(index2*len(I_sort))]\n",
        "    N_I =1.0*(N_I-I_min)/(I_max-I_min)\n",
        "    N_I[N_I>1.0]=1.0\n",
        "    N_I[N_I<0.0]=0.0\n",
        "    N_I2 = N_I.astype(np.float32)\n",
        "    return N_I2\n",
        "\n",
        "def Norm_Zscore(img):\n",
        "    img= (img-np.mean(img))/np.std(img) \n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBWv91cIdJpD"
      },
      "source": [
        "class Dataset(Data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, names,iterations,norm=True):\n",
        "        'Initialization'\n",
        "        self.names = names\n",
        "        self.norm = norm\n",
        "        self.iterations = iterations\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.names) * 2\n",
        "\n",
        "  def __getitem__(self, step):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        # print(self.names)\n",
        "        index_pair = np.random.permutation(len(self.names)) [0:4]\n",
        "        img_A = load_4D(self.names[index_pair[0]])\n",
        "        img_B = load_4D(self.names[index_pair[1]])     \n",
        "        \n",
        "        if self.norm:\n",
        "            return  Norm_Zscore(imgnorm(img_A)) , Norm_Zscore(imgnorm(img_B))\n",
        "        else:\n",
        "            return img_A, img_B\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH-wAyCn3G7Z"
      },
      "source": [
        "training_generator = Data.DataLoader(Dataset(file_names,iterations=2,norm=True), batch_size=2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EwQxlOM3HBj"
      },
      "source": [
        "ex1 = torch.rand(2, 40, 4, 4, 4)\n",
        "ex2 = ex1.flatten(start_dim=1, end_dim=4)\n",
        "ex2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t5gtYcu4IH_"
      },
      "source": [
        "for X,Y in training_generator:\n",
        "  print(X.shape)\n",
        "  print(Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcfuvlWB2-tp"
      },
      "source": [
        "# Building Affine Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDRNICPadJry"
      },
      "source": [
        "class Admir_Affine_Encoder(nn.Module):\n",
        "    def __init__(self, in_channel, start_channel, num_conv_blocks=6):\n",
        "        self.in_channel = in_channel\n",
        "        self.start_channel = start_channel\n",
        "        self.num_conv_blocks = num_conv_blocks\n",
        "        self.encoder_layer_list = []\n",
        "        super(Admir_Affine_Encoder, self).__init__()\n",
        "        self.create_model()\n",
        "\n",
        "    def affine_conv_block(self, in_channels, out_channels, kernel_size = 3, stride = 2, padding = 1, bias=True ):\n",
        "      layer = nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "                            nn.BatchNorm3d(out_channels),\n",
        "                            nn.LeakyReLU(negative_slope=0.1))\n",
        "      return layer\n",
        "    \n",
        "\n",
        "    def create_model(self):\n",
        "      for i in range(self.num_conv_blocks):\n",
        "          if(i == 0):\n",
        "            lyr = self.affine_conv_block(in_channels = self.in_channel, out_channels = self.start_channel)\n",
        "            self.encoder_layer_list.append(lyr)\n",
        "          else:\n",
        "            lyr = self.affine_conv_block(in_channels= self.start_channel * i, out_channels = self.start_channel * (i+1))\n",
        "            self.encoder_layer_list.append(lyr)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "      # print(\"x,y\", x.shape, \"  \", y.shape)\n",
        "      x_in=torch.cat((x, y), 1)\n",
        "      e0 = self.encoder_layer_list[0](x_in)\n",
        "      e1 = self.encoder_layer_list[1](e0)\n",
        "      e2 = self.encoder_layer_list[2](e1)\n",
        "      e3 = self.encoder_layer_list[3](e2)\n",
        "      e4 = self.encoder_layer_list[4](e3)\n",
        "      return e4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtqnFkbuVzwY"
      },
      "source": [
        "affine_conv_model = Admir_Affine_Encoder(in_channel=2, start_channel=8, num_conv_blocks=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMnKqsEzdJua"
      },
      "source": [
        "class Admir_Affine_Output(nn.Module):\n",
        "  def __init__(self, in_units, out_units=128, dropout_prob = 0.3):\n",
        "    \n",
        "    self.in_units = in_units\n",
        "    self.out_units = out_units\n",
        "    self.dropout_prob = dropout_prob\n",
        "    super(Admir_Affine_Output, self).__init__()\n",
        "    self.trns_ob = self.translation_output_block(self.in_units, self.out_units)\n",
        "    self.rss_ob = self.rot_scale_shear_output_block(self.in_units, self.out_units)\n",
        "    return;\n",
        "  \n",
        "  def translation_output_block(self, in_units, out_units):\n",
        "    layer = nn.Sequential(\n",
        "          nn.Linear(in_features = in_units, out_features= out_units),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units, out_features= out_units//2),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units//2, out_features= out_units//4),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units//4, out_features= out_units//8),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units//8, out_features= 3))\n",
        "    return layer\n",
        "\n",
        "  def rot_scale_shear_output_block(self, in_units, out_units):\n",
        "    layer = nn.Sequential(\n",
        "          nn.Linear(in_features = in_units, out_features= out_units),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units, out_features= out_units//2),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units//2, out_features= out_units//4),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units//4, out_features= out_units//8),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units//8, out_features= 9),\n",
        "          nn.Tanh())\n",
        "    return layer\n",
        "  \n",
        "  def forward(self, input_tnsr):\n",
        "    ip = input_tnsr.flatten(start_dim=1, end_dim=4)\n",
        "    #print(ip.shape)\n",
        "    translation_output = self.trns_ob(ip)\n",
        "    rotate_scale_shear_output = self.rss_ob(ip)\n",
        "    return [translation_output, rotate_scale_shear_output]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jyWklHIdJw-"
      },
      "source": [
        "affine_output_model = Admir_Affine_Output( in_units= 2560)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i2igZOPlj4V",
        "outputId": "c25a6b3e-746b-491d-f3e9-e893f949f909",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for X,Y in training_generator:\n",
        "  print(X.shape)\n",
        "  print(Y.shape)\n",
        "  conv_out = affine_conv_model(X, Y)\n",
        "  print(conv_out.shape)\n",
        "  output_out = affine_output_model(conv_out)\n",
        "  print(output_out[0].shape)\n",
        "  print(output_out[1].shape)\n",
        "  print(\"========== ============== =============\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 40, 4, 4, 4])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 9])\n",
            "========== ============== =============\n",
            "\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 40, 4, 4, 4])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 9])\n",
            "========== ============== =============\n",
            "\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 40, 4, 4, 4])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 9])\n",
            "========== ============== =============\n",
            "\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 40, 4, 4, 4])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 9])\n",
            "========== ============== =============\n",
            "\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 40, 4, 4, 4])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 9])\n",
            "========== ============== =============\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNrKuRLqiJYF"
      },
      "source": [
        "# Spatial Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekf9ihEilj8O"
      },
      "source": [
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    N-D Spatial Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, is_affine=False, theta = None, mode='bilinear'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        self.isaffine = is_affine\n",
        "        self.theta = theta\n",
        "        # create sampling grid\n",
        "        vectors = [torch.arange(0, s) for s in size]\n",
        "        grids = torch.meshgrid(vectors)\n",
        "        grid = torch.stack(grids)\n",
        "        grid = torch.unsqueeze(grid, 0)\n",
        "        grid = grid.type(torch.FloatTensor)\n",
        "        # registering the grid as a buffer cleanly moves it to the GPU, but it also\n",
        "        # adds it to the state dict. this is annoying since everything in the state dict\n",
        "        # is included when saving weights to disk, so the model files are way bigger\n",
        "        # than they need to be. so far, there does not appear to be an elegant solution.\n",
        "        # see: https://discuss.pytorch.org/t/how-to-register-buffer-without-polluting-state-dict\n",
        "        self.register_buffer('grid', grid)\n",
        "\n",
        "    def forward(self, src, flow):      \n",
        "      if (self.isaffine):\n",
        "        grid = F.affine_grid(self.theta, (2, 1, 128, 128, 128))\n",
        "        warped_image = F.grid_sample(src, grid)\n",
        "        return warped_image\n",
        "      else:\n",
        "        # new locations\n",
        "        new_locs = self.grid + flow\n",
        "        shape = flow.shape[2:]\n",
        "\n",
        "        # need to normalize grid values to [-1, 1] for resampler\n",
        "        for i in range(len(shape)):\n",
        "            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)\n",
        "\n",
        "        # move channels dim to last position\n",
        "        # also not sure why, but the channels need to be reversed\n",
        "        if len(shape) == 2:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 1)\n",
        "            new_locs = new_locs[..., [1, 0]]\n",
        "        elif len(shape) == 3:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 4, 1)\n",
        "            new_locs = new_locs[..., [2, 1, 0]]\n",
        "\n",
        "        return nnf.grid_sample(src, new_locs, align_corners=True, mode=self.mode)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBY7LjNtlkAS"
      },
      "source": [
        "spatial_transformer = SpatialTransformer(size=(128, 28, 128), is_affine=True, theta=torch.randn(size=(3, 4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyccNNSMVor3",
        "outputId": "58ecb096-50ae-42c7-b774-4369f1f6dca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(spatial_transformer.grid.shape)\n",
        "print(spatial_transformer.isaffine)\n",
        "print(\"========= =========== ======\")\n",
        "print()\n",
        "#print(spatial_transformer.grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 128, 28, 128])\n",
            "True\n",
            "========= =========== ======\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuqfeLHhiO8a"
      },
      "source": [
        "# Deformable ConvNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBiyQjAIlwMI"
      },
      "source": [
        "class Admir_Deformable_UNet(nn.Module):\n",
        "  def __init__(self,in_channel  , n_classes,start_channel):\n",
        "        self.in_channel = in_channel\n",
        "        self.n_classes = n_classes\n",
        "        self.start_channel = start_channel\n",
        "        super(Admir_Deformable_UNet, self).__init__()\n",
        "        self.eninput = self.encoder(self.in_channel, self.start_channel, bias=False)\n",
        "\n",
        "        self.ec1 = self.encoder(self.start_channel, self.start_channel, bias=False)\n",
        "        self.ec2 = self.encoder(self.start_channel, self.start_channel*2, stride=2, bias=False)\n",
        "\n",
        "        self.ec3 = self.encoder(self.start_channel*2, self.start_channel*2, bias=False)\n",
        "        self.ec4 = self.encoder(self.start_channel*2, self.start_channel*4, stride=2, bias=False)\n",
        "\n",
        "        self.ec5 = self.encoder(self.start_channel*4, self.start_channel*4, bias=False)\n",
        "        self.ec6 = self.encoder(self.start_channel*4, self.start_channel*8, stride=2, bias=False)\n",
        "\n",
        "       \n",
        "    \n",
        "        self.dc1 = self.encoder(self.start_channel*8, self.start_channel*8, kernel_size=3, stride=1, bias=False) \n",
        "        self.dc2 = self.encoder(self.start_channel*4, self.start_channel*4, kernel_size=3, stride=1, bias=False)          \n",
        "        self.dc3 = self.encoder(self.start_channel*2, self.start_channel*2, kernel_size=3, stride=1, bias=False)\n",
        "\n",
        "        self.up1 = self.decoder(self.start_channel*8, self.start_channel*4)\n",
        "        self.up2 = self.decoder(self.start_channel*4, self.start_channel*2)\n",
        "        self.up3 = self.decoder(self.start_channel*2, self.start_channel)\n",
        "\n",
        "        self.dc4 = self.output(self.start_channel, self.n_classes,kernel_size=1,bias=False)\n",
        "\n",
        "  def encoder(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1,\n",
        "                bias=True):\n",
        "    layer = nn.Sequential(\n",
        "                nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "                nn.BatchNorm3d(out_channels),\n",
        "                nn.LeakyReLU(negative_slope=0.1))\n",
        "    return layer\n",
        "\n",
        "  def decoder(self, in_channels, out_channels, kernel_size=2, stride=2, padding=0,\n",
        "                output_padding=0, bias=True):\n",
        "    layer = nn.Sequential(\n",
        "                nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride,\n",
        "                               padding=padding, output_padding=output_padding, bias=bias),\n",
        "            nn.LeakyReLU(negative_slope=0.1))\n",
        "    return layer\n",
        "       \n",
        "  def output(self, in_channels, out_channels, kernel_size=3, \n",
        "                bias=False, batchnorm=False):\n",
        "    layer = nn.Sequential(\n",
        "                nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias),\n",
        "               )\n",
        "    return layer\n",
        "\n",
        "  def forward(self, x,y):\n",
        "        # print(\"x,y\", x.shape, \"  \", y.shape)\n",
        "        x_in=torch.cat((x, y), 1)  \n",
        "        e0 = self.eninput(x_in)\n",
        "\n",
        "        # print(\"e0\", e0.shape)\n",
        "\n",
        "        e0 = self.ec1(e0)\n",
        "        es1 = self.ec2(e0)   #strided\n",
        "        # print(\"e0\", e0.shape)\n",
        "        # print(\"es1\", es1.shape)\n",
        "\n",
        "        e1 = self.ec3(es1)   \n",
        "        es2 = self.ec4(e1)   #strided\n",
        "        # print(\"e1\", e1.shape)\n",
        "        # print(\"es2\", es2.shape)\n",
        "\n",
        "        e2 = self.ec5(es2)\n",
        "        es3 = self.ec6(e2)   #strided\n",
        "        # print(\"e2\", e2.shape)\n",
        "        # print(\"es3\", es3.shape)\n",
        "\n",
        "        \n",
        "\n",
        "        d0 = self.dc1(es3)\n",
        "        # print(\"d0\", d0.shape)\n",
        "\n",
        "        d0 = torch.add(self.up1(d0), e2)\n",
        "        # print(\"d0\", d0.shape)\n",
        "\n",
        "        d1 = self.dc2(d0)\n",
        "        d1 = torch.add(self.up2(d1), e1)\n",
        "        # print(\"d1\", d1.shape)\n",
        "\n",
        "        d2 = self.dc3(d1)\n",
        "        d2 = torch.add(self.up3(d2), e0)\n",
        "        print(\"d2\", d2.shape)\n",
        "\n",
        "        output = self.dc4(d2)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRni_hDJBuhF",
        "outputId": "793b3bc1-dcc3-44ed-f396-f93d56cce5b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "model = Admir_Deformable_UNet(2,3,16).cuda() # assining cuda to model\n",
        "\n",
        "for X,Y in training_generator:\n",
        "  X = X.cuda().float()\n",
        "  Y = Y.cuda().float()\n",
        "  print(X.shape)\n",
        "  print(Y.shape)\n",
        "  out = model(X, Y)\n",
        "  print(out.shape)\n",
        "  print(\"========== ============== =============\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "d2 torch.Size([2, 16, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "========== ============== =============\n",
            "\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "d2 torch.Size([2, 16, 128, 128, 128])\n",
            "torch.Size([2, 3, 128, 128, 128])\n",
            "========== ============== =============\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLFbr5Iy_HME"
      },
      "source": [
        "# Loss Function NCC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A3TIKScllXG"
      },
      "source": [
        "Reference: https://github.com/yuta-hi/pytorch_similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y79_YH_Fk1vI"
      },
      "source": [
        "def normalized_cross_correlation(x, y, return_map, reduction='mean', eps=1e-8):\n",
        "    \"\"\" N-dimensional normalized cross correlation (NCC)\n",
        "    Args:\n",
        "        x (~torch.Tensor): Input tensor.\n",
        "        y (~torch.Tensor): Input tensor.\n",
        "        return_map (bool): If True, also return the correlation map.\n",
        "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
        "            ``'mean'`` | ``'sum'``. Defaults to ``'sum'``.\n",
        "        eps (float, optional): Epsilon value for numerical stability. Defaults to 1e-8.\n",
        "    Returns:\n",
        "        ~torch.Tensor: Output scalar\n",
        "        ~torch.Tensor: Output tensor\n",
        "    \"\"\"\n",
        "\n",
        "    shape = x.shape\n",
        "    b = shape[0]\n",
        "\n",
        "    # reshape\n",
        "    x = x.view(b, -1)\n",
        "    y = y.view(b, -1)\n",
        "\n",
        "    # mean\n",
        "    x_mean = torch.mean(x, dim=1, keepdim=True)\n",
        "    y_mean = torch.mean(y, dim=1, keepdim=True)\n",
        "\n",
        "    # deviation\n",
        "    x = x - x_mean\n",
        "    y = y - y_mean\n",
        "\n",
        "    dev_xy = torch.mul(x,y)\n",
        "    dev_xx = torch.mul(x,x)\n",
        "    dev_yy = torch.mul(y,y)\n",
        "\n",
        "    dev_xx_sum = torch.sum(dev_xx, dim=1, keepdim=True)\n",
        "    dev_yy_sum = torch.sum(dev_yy, dim=1, keepdim=True)\n",
        "\n",
        "    ncc = torch.div(dev_xy + eps / dev_xy.shape[1],\n",
        "                    torch.sqrt( torch.mul(dev_xx_sum, dev_yy_sum)) + eps)\n",
        "    ncc_map = ncc.view(b, *shape[1:])\n",
        "\n",
        "    # reduce\n",
        "    if reduction == 'mean':\n",
        "        ncc = torch.mean(torch.sum(ncc, dim=1))\n",
        "    elif reduction == 'sum':\n",
        "        ncc = torch.sum(ncc)\n",
        "    else:\n",
        "        raise KeyError('unsupported reduction type: %s' % reduction)\n",
        "\n",
        "    if not return_map:\n",
        "        return ncc\n",
        "\n",
        "    return ncc, ncc_map\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Ck9sjlk55k"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NormalizedCrossCorrelation(nn.Module):\n",
        "    \"\"\" N-dimensional normalized cross correlation (NCC)\n",
        "    Args:\n",
        "        eps (float, optional): Epsilon value for numerical stability. Defaults to 1e-8.\n",
        "        return_map (bool, optional): If True, also return the correlation map. Defaults to False.\n",
        "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
        "            ``'mean'`` | ``'sum'``. Defaults to ``'mean'``.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 eps=1e-8,\n",
        "                 return_map=False,\n",
        "                 reduction='mean'):\n",
        "\n",
        "        super(NormalizedCrossCorrelation, self).__init__()\n",
        "\n",
        "        self._eps = eps\n",
        "        self._return_map = return_map\n",
        "        self._reduction = reduction\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        return normalized_cross_correlation(x, y,self._return_map, self._reduction, self._eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMVRffCTpyTm",
        "outputId": "35100a73-7092-4713-ef32-fddab26a95c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#  Checking NCC loss\n",
        "\n",
        "similarity_loss = NormalizedCrossCorrelation()\n",
        "for X,Y in training_generator:\n",
        "  X = X.cuda().float()\n",
        "  Y = Y.cuda().float()\n",
        "  print(X.shape)\n",
        "  print(Y.shape)\n",
        "  out = similarity_loss(X, Y)\n",
        "  print(out)\n",
        "  print(\"========== ============== =============\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "tensor(0.7159, device='cuda:0')\n",
            "========== ============== =============\n",
            "\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "tensor(0.7159, device='cuda:0')\n",
            "========== ============== =============\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEnYx3fxzvgQ"
      },
      "source": [
        "# Regularizer - DVF edge smoothness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dwi3Pzcz1uj",
        "outputId": "8dff34ad-0052-448b-ab6c-233f7409cb1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = torch.randn(size=(4, 3, 128, 128, 128))\n",
        "sobel = [[1, 2, 1], [0, 0, 0], [-1, -2, -1]]\n",
        "depth=x.size()[1]\n",
        "channels=x.size()[2]\n",
        "\n",
        "print(depth)\n",
        "print(channels)\n",
        "print(sobel)\n",
        "print()\n",
        "sobel_kernel = torch.FloatTensor(sobel).expand(depth,channels,3,3).unsqueeze(0)\n",
        "print(sobel_kernel.shape) \n",
        "malignacy = F.conv3d(x, sobel_kernel, stride=1, padding=1)\n",
        "print(malignacy.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "128\n",
            "[[1, 2, 1], [0, 0, 0], [-1, -2, -1]]\n",
            "\n",
            "torch.Size([1, 3, 128, 3, 3])\n",
            "torch.Size([4, 1, 3, 128, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C7TYQhGz1xb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP-_ZHtLz1zp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raicgKapz12a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}