{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADMIR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzKXkbFB6bPEFRABHTOH8l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surajsrivathsa/image_registration/blob/main/ADMIR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uENFUaxrdBRN"
      },
      "source": [
        "# Preamble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_QQG0blca17",
        "outputId": "72cc6fc8-d9c6-4604-f42f-a7f50eccd5c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install --upgrade nibabel"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nibabel\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/7f/d3c29792fae50ef4f1f8f87af8a94d5d9fe76550b86ebcf8a251110169d8/nibabel-3.2.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging>=14.3 in /usr/local/lib/python3.6/dist-packages (from nibabel) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from nibabel) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from packaging>=14.3->nibabel) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=14.3->nibabel) (2.4.7)\n",
            "Installing collected packages: nibabel\n",
            "  Found existing installation: nibabel 3.0.2\n",
            "    Uninstalling nibabel-3.0.2:\n",
            "      Successfully uninstalled nibabel-3.0.2\n",
            "Successfully installed nibabel-3.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rT9Nd7ScgDi",
        "outputId": "3c54193f-8fdf-4308-e8f7-26bcf02a8419",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-x6D4OnafOj",
        "outputId": "1e96e638-b49a-4694-bf7a-a6df23016aae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import nibabel as nb\n",
        "import os, sys, glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "print(\"nibabel version: {}\".format(nb.__version__))\n",
        "print(\"pytorch version: {}\".format(torch.__version__))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nibabel version: 3.2.0\n",
            "pytorch version: 1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O09OgvEJc_CM"
      },
      "source": [
        "t1_fn = '/content/drive/My Drive/Colab Notebooks/image_registration/T1_ixi/IXI002-Guys-0828-T1.nii.gz'\n",
        "t2_fn = '/content/drive/My Drive/Colab Notebooks/image_registration/T2_ixi/IXI002-Guys-0828-T2.nii.gz'  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWcFQJ_Sc-9p"
      },
      "source": [
        "data_path = \"/content/drive/My Drive/Colab Notebooks/image_registration/resampled_mri/admir_data\"\n",
        "file_names = glob.glob(os.path.join(data_path, \"*.nii.gz\"))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpv76iqJoJ3C",
        "outputId": "ca36bb0e-52b5-4b46-8a80-7c2ba4e84a6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(file_names)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAVJGb_KdFbx"
      },
      "source": [
        "# Image Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aKmg-UYdJmK",
        "outputId": "e4683da2-30fd-4bfe-daac-1e3abfd07334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img_nb1 = nb.load(file_names[0])\n",
        "img_nb1.shape\n",
        "img_nb2 = nb.load(file_names[1])\n",
        "img_nb2.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 128, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1Q1VnK3rD-n"
      },
      "source": [
        "def load_4D(name):\n",
        "    X_nb = nb.load(name)\n",
        "    X_np = X_nb.dataobj\n",
        "    X_np = np.reshape(X_np, (1,)+ X_np.shape)\n",
        "    return X_np\n",
        "\n",
        "def imgnorm(N_I,index1=0.0001,index2=0.0001):\n",
        "    I_sort = np.sort(N_I.flatten())\n",
        "    I_min = I_sort[int(index1*len(I_sort))]\n",
        "    I_max = I_sort[-int(index2*len(I_sort))]\n",
        "    N_I =1.0*(N_I-I_min)/(I_max-I_min)\n",
        "    N_I[N_I>1.0]=1.0\n",
        "    N_I[N_I<0.0]=0.0\n",
        "    N_I2 = N_I.astype(np.float32)\n",
        "    return N_I2\n",
        "\n",
        "def Norm_Zscore(img):\n",
        "    img= (img-np.mean(img))/np.std(img) \n",
        "    return img"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBWv91cIdJpD"
      },
      "source": [
        "class Dataset(Data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, names,iterations,norm=True):\n",
        "        'Initialization'\n",
        "        self.names = names\n",
        "        self.norm = norm\n",
        "        self.iterations = iterations\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.names) * 2\n",
        "\n",
        "  def __getitem__(self, step):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        # print(self.names)\n",
        "        index_pair = np.random.permutation(len(self.names)) [0:4]\n",
        "        img_A = load_4D(self.names[index_pair[0]])\n",
        "        img_B = load_4D(self.names[index_pair[1]])     \n",
        "        \n",
        "        if self.norm:\n",
        "            return  Norm_Zscore(imgnorm(img_A)) , Norm_Zscore(imgnorm(img_B))\n",
        "        else:\n",
        "            return img_A, img_B\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH-wAyCn3G7Z"
      },
      "source": [
        "training_generator = Data.DataLoader(Dataset(file_names,iterations=2,norm=True), batch_size=2, shuffle=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EwQxlOM3HBj",
        "outputId": "d02307e7-67ed-4710-b449-5fd473a83aee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ex1 = torch.rand(2, 40, 4, 4, 4)\n",
        "ex2 = ex1.flatten(start_dim=1, end_dim=4)\n",
        "ex2.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2560])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t5gtYcu4IH_",
        "outputId": "6365335e-626b-4a0e-bf4c-57c7b7f3059c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for X,Y in training_generator:\n",
        "  print(X.shape)\n",
        "  print(Y.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcfuvlWB2-tp"
      },
      "source": [
        "# Building Affine Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDRNICPadJry"
      },
      "source": [
        "class Admir_Affine_Encoder(nn.Module):\n",
        "    def __init__(self, in_channel, start_channel, num_conv_blocks=6):\n",
        "        self.in_channel = in_channel\n",
        "        self.start_channel = start_channel\n",
        "        self.num_conv_blocks = num_conv_blocks\n",
        "        self.encoder_layer_list = []\n",
        "        super(Admir_Affine_Encoder, self).__init__()\n",
        "        self.create_model()\n",
        "\n",
        "    def affine_conv_block(self, in_channels, out_channels, kernel_size = 3, stride = 2, padding = 1, bias=True ):\n",
        "      layer = nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "                            nn.BatchNorm3d(out_channels),\n",
        "                            nn.LeakyReLU(negative_slope=0.1))\n",
        "      return layer\n",
        "    \n",
        "\n",
        "    def create_model(self):\n",
        "      for i in range(self.num_conv_blocks):\n",
        "          if(i == 0):\n",
        "            lyr = self.affine_conv_block(in_channels = self.in_channel, out_channels = self.start_channel)\n",
        "            self.encoder_layer_list.append(lyr)\n",
        "          else:\n",
        "            lyr = self.affine_conv_block(in_channels= self.start_channel * i, out_channels = self.start_channel * (i+1))\n",
        "            self.encoder_layer_list.append(lyr)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "      # print(\"x,y\", x.shape, \"  \", y.shape)\n",
        "      x_in=torch.cat((x, y), 1)\n",
        "      e0 = self.encoder_layer_list[0](x_in)\n",
        "      e1 = self.encoder_layer_list[1](e0)\n",
        "      e2 = self.encoder_layer_list[2](e1)\n",
        "      e3 = self.encoder_layer_list[3](e2)\n",
        "      e4 = self.encoder_layer_list[4](e3)\n",
        "      return e4\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtqnFkbuVzwY"
      },
      "source": [
        "affine_conv_model = Admir_Affine_Encoder(in_channel=2, start_channel=8, num_conv_blocks=5)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMnKqsEzdJua"
      },
      "source": [
        "class Admir_Affine_Output(nn.Module):\n",
        "  def __init__(self, in_units, out_units=128, dropout_prob = 0.3):\n",
        "    \n",
        "    self.in_units = in_units\n",
        "    self.out_units = out_units\n",
        "    self.dropout_prob = dropout_prob\n",
        "    super(Admir_Affine_Output, self).__init__()\n",
        "    self.trns_ob = self.translation_output_block(self.in_units, self.out_units)\n",
        "    self.rss_ob = self.rot_scale_shear_output_block(self.in_units, self.out_units)\n",
        "    return;\n",
        "  \n",
        "  def translation_output_block(self, in_units, out_units):\n",
        "    layer = nn.Sequential(\n",
        "          nn.Linear(in_features = in_units, out_features= out_units),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units, out_features= out_units//2),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units//2, out_features= out_units//4),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units//4, out_features= out_units//8),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units//8, out_features= 3))\n",
        "    return layer\n",
        "\n",
        "  def rot_scale_shear_output_block(self, in_units, out_units):\n",
        "    layer = nn.Sequential(\n",
        "          nn.Linear(in_features = in_units, out_features= out_units),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units, out_features= out_units//2),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units//2, out_features= out_units//4),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units//4, out_features= out_units//8),\n",
        "          nn.Dropout(p=self.dropout_prob),\n",
        "          nn.Linear(in_features=out_units//8, out_features= 9),\n",
        "          nn.Tanh())\n",
        "    return layer\n",
        "  \n",
        "  def forward(self, input_tnsr):\n",
        "    ip = input_tnsr.flatten(start_dim=1, end_dim=4)\n",
        "    #print(ip.shape)\n",
        "    translation_output = self.trns_ob(ip)\n",
        "    rotate_scale_shear_output = self.rss_ob(ip)\n",
        "    return [translation_output, rotate_scale_shear_output]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jyWklHIdJw-"
      },
      "source": [
        "affine_output_model = Admir_Affine_Output( in_units= 2560)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i2igZOPlj4V",
        "outputId": "c2fd6331-b59c-4c3d-c4e1-f8f0a54f83a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Suraj: Added spatial transformation code for warping\n",
        "for X,Y in training_generator:\n",
        "  print(X.shape)\n",
        "  print(Y.shape)\n",
        "  conv_out = affine_conv_model(X, Y)\n",
        "  print(conv_out.shape)\n",
        "  output_out = affine_output_model(conv_out)\n",
        "  print(output_out[0].shape)\n",
        "  print(output_out[1].shape)\n",
        "  affine_tnsr = torch.cat((output_out[1], output_out[0]), 1)\n",
        "  theta = torch.reshape(affine_tnsr, (2, 3, 4))\n",
        "  print(\"========== ============== =============\")\n",
        "  print(theta.shape)\n",
        "  print(theta)\n",
        "  print(\"========== ============== =============\")\n",
        "  grid = F.affine_grid(theta, (2, 1, 128, 128, 128))\n",
        "  print(grid.shape)\n",
        "  #print(grid)\n",
        "  warped_image = F.grid_sample(Y, grid)\n",
        "  print(\"========== ============== =============\")\n",
        "  print()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 40, 4, 4, 4])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 9])\n",
            "========== ============== =============\n",
            "torch.Size([2, 3, 4])\n",
            "tensor([[[-0.1220, -0.0160,  0.0155,  0.2409],\n",
            "         [-0.1245,  0.1718,  0.3042,  0.2036],\n",
            "         [ 0.3949,  0.1187,  0.0862,  0.1176]],\n",
            "\n",
            "        [[ 0.1460, -0.1198,  0.0899,  0.1257],\n",
            "         [-0.0433, -0.0366,  0.3735, -0.1034],\n",
            "         [ 0.3608, -0.0035,  0.1467,  0.1008]]], grad_fn=<ViewBackward>)\n",
            "========== ============== =============\n",
            "torch.Size([2, 128, 128, 128, 3])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3448: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3385: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "========== ============== =============\n",
            "\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 40, 4, 4, 4])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 9])\n",
            "========== ============== =============\n",
            "torch.Size([2, 3, 4])\n",
            "tensor([[[-0.0956,  0.1057,  0.1511,  0.1364],\n",
            "         [ 0.0248,  0.1241,  0.1382,  0.1282],\n",
            "         [ 0.2615, -0.0539, -0.1083,  0.1467]],\n",
            "\n",
            "        [[ 0.0624,  0.0283, -0.0429,  0.3442],\n",
            "         [ 0.0137,  0.1154,  0.1751,  0.0942],\n",
            "         [ 0.2517,  0.2129,  0.2051, -0.0048]]], grad_fn=<ViewBackward>)\n",
            "========== ============== =============\n",
            "torch.Size([2, 128, 128, 128, 3])\n",
            "========== ============== =============\n",
            "\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 40, 4, 4, 4])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 9])\n",
            "========== ============== =============\n",
            "torch.Size([2, 3, 4])\n",
            "tensor([[[ 0.0196, -0.0208,  0.0681,  0.1238],\n",
            "         [-0.0448,  0.1987,  0.2191, -0.0853],\n",
            "         [ 0.2400,  0.0952,  0.1087,  0.1642]],\n",
            "\n",
            "        [[ 0.1690, -0.0879, -0.0913,  0.1474],\n",
            "         [ 0.1242,  0.1627,  0.4899,  0.1611],\n",
            "         [ 0.5175,  0.1139,  0.0516, -0.1211]]], grad_fn=<ViewBackward>)\n",
            "========== ============== =============\n",
            "torch.Size([2, 128, 128, 128, 3])\n",
            "========== ============== =============\n",
            "\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 40, 4, 4, 4])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 9])\n",
            "========== ============== =============\n",
            "torch.Size([2, 3, 4])\n",
            "tensor([[[-0.0568,  0.1738,  0.2747,  0.1448],\n",
            "         [-0.0465,  0.2415,  0.3905,  0.1098],\n",
            "         [ 0.2336,  0.1550,  0.0362,  0.0033]],\n",
            "\n",
            "        [[ 0.0723,  0.1691,  0.0961,  0.2341],\n",
            "         [-0.0235,  0.0877,  0.2575, -0.2066],\n",
            "         [ 0.2545, -0.1392,  0.4285,  0.2631]]], grad_fn=<ViewBackward>)\n",
            "========== ============== =============\n",
            "torch.Size([2, 128, 128, 128, 3])\n",
            "========== ============== =============\n",
            "\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 1, 128, 128, 128])\n",
            "torch.Size([2, 40, 4, 4, 4])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 9])\n",
            "========== ============== =============\n",
            "torch.Size([2, 3, 4])\n",
            "tensor([[[-0.1376, -0.0159,  0.0842,  0.0249],\n",
            "         [-0.0528,  0.0605,  0.2418,  0.1270],\n",
            "         [ 0.3774,  0.0160,  0.1472,  0.1068]],\n",
            "\n",
            "        [[ 0.0506, -0.0369,  0.1164,  0.1140],\n",
            "         [ 0.0765,  0.1213,  0.3513,  0.1346],\n",
            "         [ 0.2528, -0.1645,  0.1513,  0.1981]]], grad_fn=<ViewBackward>)\n",
            "========== ============== =============\n",
            "torch.Size([2, 128, 128, 128, 3])\n",
            "========== ============== =============\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekf9ihEilj8O"
      },
      "source": [
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    N-D Spatial Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, mode='bilinear'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        # create sampling grid\n",
        "        vectors = [torch.arange(0, s) for s in size]\n",
        "        grids = torch.meshgrid(vectors)\n",
        "        grid = torch.stack(grids)\n",
        "        grid = torch.unsqueeze(grid, 0)\n",
        "        grid = grid.type(torch.FloatTensor)\n",
        "\n",
        "        # registering the grid as a buffer cleanly moves it to the GPU, but it also\n",
        "        # adds it to the state dict. this is annoying since everything in the state dict\n",
        "        # is included when saving weights to disk, so the model files are way bigger\n",
        "        # than they need to be. so far, there does not appear to be an elegant solution.\n",
        "        # see: https://discuss.pytorch.org/t/how-to-register-buffer-without-polluting-state-dict\n",
        "        self.register_buffer('grid', grid)\n",
        "\n",
        "    def forward(self, src, flow):\n",
        "        # new locations\n",
        "        new_locs = self.grid + flow\n",
        "        shape = flow.shape[2:]\n",
        "\n",
        "        # need to normalize grid values to [-1, 1] for resampler\n",
        "        for i in range(len(shape)):\n",
        "            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)\n",
        "\n",
        "        # move channels dim to last position\n",
        "        # also not sure why, but the channels need to be reversed\n",
        "        if len(shape) == 2:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 1)\n",
        "            new_locs = new_locs[..., [1, 0]]\n",
        "        elif len(shape) == 3:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 4, 1)\n",
        "            new_locs = new_locs[..., [2, 1, 0]]\n",
        "\n",
        "        return nnf.grid_sample(src, new_locs, align_corners=True, mode=self.mode)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBY7LjNtlkAS"
      },
      "source": [
        "spatial_transformer = SpatialTransformer(size=(128, 28, 128))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyccNNSMVor3",
        "outputId": "6275d9e4-ea8e-4e16-ac6f-a185617aa344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(spatial_transformer.grid.shape)\n",
        "print(\"========= =========== ======\")\n",
        "print()\n",
        "#print(spatial_transformer.grid)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 128, 28, 128])\n",
            "========= =========== ======\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14AQ14D5ZnsP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}